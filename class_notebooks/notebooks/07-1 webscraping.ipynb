{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HTML Refresher\n",
    "This part is based on chapter 11 of *Automate the Boring Stuff with Python* by Al Sweigart\n",
    "\n",
    "HTML files are plain text files containing *tags*, which are words enclosed in angle brackets. Tags tell the browser how to format the web page. A starting tag and closing tag can enclose some text to form an element. The text (or inner HTML) is the content between the starting and closing tags.\n",
    "\n",
    "There are many different tags in HTML. Some of these tags have extra properties in the form of attributes within the angle brackets. For example, the `<a>` tag encloses text that should be a link.\n",
    "\n",
    "Some elements have an `id` attribute that is used to uniquely identify the element in the page. You will often instruct your programs to seek out an element by its id attribute, so figuring out an element’s id attribute using the browser’s developer tools is a common task in writing web scraping programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << EOF > data/example.html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Hello!</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Hello World!</h1>\n",
    "You are extremely welcome!<br>\n",
    "<br>\n",
    "The <a href=\\\"https://github.com/datsoftlyngby/dat4sem2019spring-python-materials\\\">Lecture Notes</a>.<br>\n",
    "<div>\n",
    "<p>paragraph 1</p>\n",
    "<p>and paragraph 2: <span id=\"span01\">This is span 1</span id=\"span02\"><span id=\"span03\">Second span element</span>\n",
    "<span class=\"red_border\">Here is the third span</span>\n",
    "</p>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\r\n",
      "<html>\r\n",
      "<head>\r\n",
      "<title>Hello!</title>\r\n",
      "</head>\r\n",
      "<body>\r\n",
      "<h1>Hello World!</h1>\r\n",
      "You are extremely welcome!<br>\r\n",
      "<br>\r\n",
      "The <a href=\\\"https://github.com/datsoftlyngby/dat4sem2019spring-python-materials\\\">Lecture Notes</a>.<br>\r\n",
      "<div>\r\n",
      "<p>paragraph 1</p>\r\n",
      "<p>and paragraph 2: <span id=\"span01\">This is span 1</span id=\"span02\"><span id=\"span03\">Second span element</span>\r\n",
      "<span class=\"red_border\">Here is the third span</span>\r\n",
      "</p>\r\n",
      "</div>\r\n",
      "</body>\r\n",
      "</html>\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# View a Page's HTML Sources\n",
    "\n",
    "Here, I will only describe how to use Firefox' development features.\n",
    "\n",
    "To view a page's sources right click on it and choose **View page source** which opens a new tab with the HTML sources.\n",
    "\n",
    "<img src=\"images/view_source_small.png\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Firefox, you can bring up the Web Developer Tools Inspector by pressing `CTRL-SHIFT-C` on Windows and Linux or by `CMD-OPTION-C` on OS X.\n",
    "\n",
    "<img src=\"images/inspector_small.png\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing HTML with BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a module for parsing and extracting information from HTML sources. The module’s name is bs4. In case it is not already installed on your machine:\n",
    "- install it with `pip install beautifulsoup4`. While beautifulsoup4 is the name used for installation, \n",
    "- to import BeautifulSoup you have to use `import bs4`.\n",
    "\n",
    "According to its documentation (https://www.crummy.com/software/BeautifulSoup/) *\"Beautiful Soup parses anything you give it, and does the tree traversal stuff for you. You can tell it \"Find all the links\", or \"Find all the links of class externalLink\", or \"Find all the links whose urls match \"foo.com\", or \"Find the table heading that's got bold text, then give me that text.\"\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a BeautifulSoup Object from a Local HTML File\n",
    "\n",
    "- The `bs4.BeautifulSoup()` function needs to be called with a string containing the HTML it will parse. \n",
    "- The `bs4.BeautifulSoup()` function returns is a `BeautifulSoup` object.\n",
    "\n",
    "You can load a local HTML file and pass a file object to `bs4.BeautifulSoup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Hello!\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   Hello World!\n",
      "  </h1>\n",
      "  You are extremely welcome!\n",
      "  <br/>\n",
      "  <br/>\n",
      "  The\n",
      "  <a href='\\\"https://github.com/datsoftlyngby/dat4sem2019spring-python-materials\\\"'>\n",
      "   Lecture Notes\n",
      "  </a>\n",
      "  .\n",
      "  <br/>\n",
      "  <div>\n",
      "   <p>\n",
      "    paragraph 1\n",
      "   </p>\n",
      "   <p>\n",
      "    and paragraph 2:\n",
      "    <span id=\"span01\">\n",
      "     This is span 1\n",
      "    </span>\n",
      "    <span id=\"span03\">\n",
      "     Second span element\n",
      "    </span>\n",
      "    <span class=\"red_border\">\n",
      "     Here is the third span\n",
      "    </span>\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "with open('./data/example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html)\n",
    "print(type(soup))\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a BeautifulSoup Object from a Remote HTML File\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <link href=\"https://github.githubassets.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars0.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars1.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars2.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars3.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://github-cloud.s3.amazonaws.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://user-images.githubusercontent.com/\" rel=\"dns-prefetch\"/>\n",
      "  <link crossorigin=\"anonymous\" href=\"https://github.githubassets.com/assets/frameworks-60c9b31633c996f9bf421aae83b644dd.css\" integrity=\"sha512-YMmzFjPJlvm/Qhqug7ZE3UZYDXl4sfGs/xmLIlbN/cPFV/AU5/89DOLsHIwlpQFyji5oVuWE049paRcr+RF3cQ==\" media=\"all\" rel=\"stylesheet\">\n",
      "   <link crossorigin=\"anonymous\" href=\"https://github.githubassets.com/assets/site-dfba4b408f2494358f8d655558507d21.css\" integrity=\"sha512-37pLQI8klDWPjWVVWFB9ITJLwVTTkp3Rt4bVf+yixrViURK9OoGHEJDbTLxBv/rTJhsLm8pb00H2H5AG3hUJfg==\" media=\"all\" rel=\"stylesheet\">\n",
      "    <link crossorigin=\"anonymous\" href=\"https://github.githubassets.com/assets/github-9c4d38797163cb606c8d76f479b3fc55.css\" integrity=\"sha512-nE04eXFjy2BsjXb0ebP8VWbZyNeZ9IUeYYega6ljSbewyx2t5uTOhbSn5ckJiM6soO/6NbmY6P1wGm+1oKoybg==\" media=\"all\" rel=\"stylesheet\">\n",
      "     <meta content=\"width=device-width\" name=\"viewport\"/>\n",
      "     <title>\n",
      "      GitHub - datsoftlyngby/dat4sem2020spring-python\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('https://github.com/datsoftlyngby/dat4sem2020spring-python')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "print(soup.prettify()[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding an Element with the `select()` Method\n",
    "\n",
    "You can retrieve HTML elements from a `BeautifulSoup` object by calling the `select()` method and passing a string of a CSS selector for the element you are looking for. Selectors are like regular expressions: They specify a pattern to look for, in this case, in HTML pages instead of general text strings.\n",
    "\n",
    "Common CSS selector patterns include:\n",
    "\n",
    "  * `soup.select('div')` ... selects all elements named `<div>`\n",
    "  * `soup.select('#lecturer')`  ... selects the element with an id attribute of author\n",
    "  * `soup.select('.notice')` ... selects all elements that use a CSS class attribute named notice\n",
    "  * `soup.select('div span')` ... selects all elements named `<span>` that are within an element named `<div>`\n",
    "  * `soup.select('div > span')` ... selects all elements named `<span>` that are directly within an element named `<div>`, with no other element in between\n",
    "  * `soup.select('input[name]')` ... selects all elements named `<input>` that have a name attribute with any value\n",
    "  * `soup.select('input[type=\"button\"]')` ... selects all elements named `<input>` that have an attribute named type with value button\n",
    "  \n",
    "See more in the documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: return type of select() <class 'bs4.element.ResultSet'>\n",
      "2: length of the returned list 1\n",
      "3: type of elements in the list <class 'bs4.element.Tag'>\n",
      "4: get text from the element \n",
      "Hello World!\n",
      "You are extremely welcome!\n",
      "5: string representation of an element:  <body>\n",
      "<h1>Hello World!</h1>\n",
      "You are extremely welcome!<br/>\n",
      "<br/>\n",
      "The <a href='\\\"https://github.com/datsoftlyngby/dat4sem2019spring-python-materials\\\"'>Lecture Notes</a>.<br/>\n",
      "<div>\n",
      "<p>paragraph 1</p>\n",
      "<p>and paragraph 2: <span id=\"span01\">This is span 1</span><span id=\"span03\">Second span element</span>\n",
      "<span class=\"red_border\">Here is the third span</span>\n",
      "</p>\n",
      "</div>\n",
      "</body>\n",
      "6: the attributes of the element:  {}\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "\n",
    "with open('./example.html') as f:\n",
    "    example_html = f.read()\n",
    "\n",
    "soup = bs4.BeautifulSoup(example_html, 'html.parser')\n",
    "\n",
    "elems = soup.select('body')\n",
    "\n",
    "#print(soup.prettify())\n",
    "print('1: return type of select()',type(elems))\n",
    "print('2: length of the returned list',len(elems))\n",
    "print('3: type of elements in the list',type(elems[0]))\n",
    "print('4: get text from the element',elems[0].getText()[:40])\n",
    "print('5: string representation of an element: ',str(elems[0]))\n",
    "print('6: the attributes of the element: ',elems[0].attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>paragraph 1</p>\n",
      "paragraph 1\n",
      "------------\n",
      "<p>and paragraph 2: <span id=\"span01\">This is span 1</span><span id=\"span03\">Second span element</span>\n",
      "<span class=\"red_border\">Here is the third span</span>\n",
      "</p>\n",
      "and paragraph 2: This is span 1Second span element\n",
      "Here is the third span\n",
      "\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "p_elems = soup.select('p')\n",
    "\n",
    "for el in p_elems:\n",
    "    print(str(el))\n",
    "    print(el.getText())\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting Data from an Element’s Attributes\n",
    "\n",
    "The `get()` method for Tag objects makes it simple to access attribute values from an element. The method is passed a string of an attribute name and returns that attribute’s value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lecturer'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'id': 'lecturer'}.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span id=\"span01\">This is span 1</span>\n",
      "span01\n",
      "True\n",
      "{'id': 'span01'}\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "with open('./example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html, 'html.parser')\n",
    "# soup.find_all?\n",
    "span_elem = soup.select('span')[0]\n",
    "print(str(span_elem))\n",
    "print(span_elem.get('id'))\n",
    "print(span_elem.get('some_nonexistent_addr') == None)\n",
    "print(span_elem.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the difference between the `select` and the `find`/`find_all` functions?\n",
    "\n",
    "You are not the first ones wondering about this... See:\n",
    "https://stackoverflow.com/questions/38028384/beautifulsoup-is-there-a-difference-between-find-and-select-python-3-x#38033910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Scraping Events from a Page\n",
    "\n",
    "\n",
    "Ususally, you will use web scraping to collect information, which you cannot gather otherwise. \n",
    "For example, let's imagine we want to do some statistics about:\n",
    "- concerts in Copenhagen, \n",
    "- their start times and \n",
    "- their door prices.\n",
    "\n",
    "Since we cannot find an API or any other open dataset, we decide to scrape the publicly available homepage www.kultunaut.dk, \n",
    "\n",
    "The website lists all possible events in Denmark. \n",
    "Concerts in Copenhagen are for example accessible here: \n",
    "- http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik\n",
    "\n",
    "**OBS** Many web pages are not built to support high traffic or they exlicitely discourage automatic access. Keep this in mind when writing your scraping tool.\n",
    "- from time import sleep\n",
    "- sleep(3) # sleep 3 seconds\n",
    "\n",
    "\n",
    "Considering our example:\n",
    "- we have to first figure out how many events there are at all. \n",
    "- We need this information, as events are given paginated, i.e., twenty events per page.\n",
    "- The link given above only returns the link to the first page with the first twenty events. \n",
    "- Out of the total amount of events we can generate the URLs for the subsequent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of events: 54\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Classical\n",
      "        </span>\n",
      "<h3><strong>Københavns Kammerkor øver</strong></h3>\n",
      "</div>\n",
      "\n",
      "\n",
      "[<strong>Københavns Kammerkor øver</strong>, <strong>Sommer i Haveselskabet: Signe Svendsen</strong>, <strong>Copenhagen Gospel Days 2020</strong>, <strong>Sommer I Haveselskabet: Mads Langer</strong>, <strong>Orgelkoncert med Jakob Lorentzen</strong>, <strong>Det Italienske Barokorgel</strong>, <strong>Alle de kvinder vi er</strong>, <strong>Sommer I Haveselskabet: Steen Jørgensen</strong>, <strong>Copenhagen Gospel Days 2020</strong>, <strong>Sommer i Haveselskabet: Barbara Moleko</strong>, <strong>Sommer i Haveselskabet: Halberg &amp; Friends</strong>, <strong>Nicklas Sørensen + Lund &amp; Pultz Melbye // ALICE Summer Series</strong>]\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&ArrStartdato=20%2F7+2020&ArrSlutdato=3%2F8+2020&Genre=Musik')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "res = soup.select('.result-count > strong:nth-child(2)')[0].text\n",
    "no_of_events = res\n",
    "print('no of events: {}'.format(no_of_events))\n",
    "\n",
    "\n",
    "elems = soup.findAll('div',{'class':'arr-genre'})\n",
    "print(elems[0])\n",
    "print();print()\n",
    "# annother notation\n",
    "elems = soup.select('div[class=arr-genre] > h3 > strong')\n",
    "print(elems)\n",
    "#print(b_el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the browser inspector pane:\n",
    "\n",
    "<img src=\"images/inspect_element.png\" width=\"500\">\n",
    "\n",
    "We can see that the desired element is hiding in a structure like: a b-tag inside a h3-tag inside a td-tag or:\n",
    "- `('td h3 b')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Københavns Kammerkor øver\n",
      "Sommer i Haveselskabet: Signe Svendsen\n",
      "Copenhagen Gospel Days 2020\n",
      "Sommer I Haveselskabet: Mads Langer\n",
      "Orgelkoncert med Jakob Lorentzen\n",
      "Det Italienske Barokorgel\n",
      "Alle de kvinder vi er\n",
      "Sommer I Haveselskabet: Steen Jørgensen\n",
      "Copenhagen Gospel Days 2020\n",
      "Sommer i Haveselskabet: Barbara Moleko\n",
      "Sommer i Haveselskabet: Halberg & Friends\n",
      "Nicklas Sørensen + Lund & Pultz Melbye // ALICE Summer Series\n"
     ]
    }
   ],
   "source": [
    "# use select with css-selectors rather than find_all\n",
    "import bs4\n",
    "import requests\n",
    "html = requests.get('http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&ArrStartdato=20%2F7+2020&ArrSlutdato=3%2F8+2020&Genre=Musik')\n",
    "txt = html.text\n",
    "soup = bs4.BeautifulSoup(txt, 'html.parser')\n",
    "events = soup.select('div[class=arr-genre] > h3 > strong')\n",
    "for e in events:\n",
    "    print(e.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <!-- Initalize title and data source variables -->\n",
      " <head>\n",
      "  <!--\n",
      "\n",
      "------------------------\n",
      "https://analytics.usa.gov/data/\n",
      "https://open.gsa.gov/api/dap/\n",
      "https://analytics.usa.gov/data/live/all-pages-realtime.csv\n",
      "https://analytics.usa.gov/data/live/all-domains-30-days.csv\n",
      "https://www.digitalgov.gov/services/dap/\n",
      "https://www.digitalgov.gov/services/dap/common-questions-about-dap-faq/#part-4\n",
      "https://support.google.com/analytics/answer/2763052?hl=en\n",
      "https://analytics.usa.gov/data/live/second-level-domains.csv\n",
      "https://analytics.usa.gov/data/live/sites.csv\n",
      "https://analytics.usa.gov/data/\n",
      "https://open.gsa.gov/api/dap/\n",
      "https://github.com/GSA/analytics.usa.gov/issues\n",
      "https://github.com/GSA/analytics.usa.gov\n",
      "https://github.com/18F/analytics-reporter\n",
      "https://www.digitalgov.gov/services/dap/\n",
      "https://cloud.gov/\n"
     ]
    }
   ],
   "source": [
    "# Get all the links in a document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "gov = requests.get('https://analytics.usa.gov')\n",
    "soup = BeautifulSoup(gov.text, 'lxml')\n",
    "print(soup.prettify()[:100])\n",
    "print('------------------------')\n",
    "for link in soup.find_all('a'):\n",
    "    if not link.get('href').startswith('https'):\n",
    "        continue\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we can scrape the events per page. Observe, that now, our `base_url` http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?Startnr={}&showmap=&Area=Kbh.%20og%20Frederiksberg&periode=&Genre=Musik& has a placeholder for the paginated results (`Startnr=`).\n",
    "\n",
    "Consequently, we scrape each page separately, see the function on the next slide: `scrape_events_per_page`. From examining the page's source code, we know that events are all given as table entries with a corresponding header. We iterate over each of the table cells and extract the strings for dates and prices if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:01,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Classical\n",
      "        </span>\n",
      "<h3><strong>Københavns Kammerkor øver</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span></span></div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Mon 20 Jul 2020 6 pm, Davidskirken</time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"http://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Classical\n",
      "        </span>\n",
      "<h3><strong>Københavns Kammerkor øver</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span></span></div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Mon 20 Jul 2020 6 pm, Davidskirken</time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"http://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Classical\n",
      "        </span>\n",
      "<h3><strong>Københavns Kammerkor øver</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span></span></div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Mon 20 Jul 2020 6 pm, Davidskirken</time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"http://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "    \n",
    "def scrape_events_per_page(url):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        A list of tuples of strings holding title, place, date, and price\n",
    "        for concerts in Copenhagen scraped from Kulturnaut.dk\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    event_cells = soup.find_all('div', {'class' : 'arr-info'})\n",
    "    #print('size',len(event_cells))\n",
    "    print(event_cells[0])\n",
    "    scraped_events_per_page = []\n",
    "    \n",
    "    for event_cell in event_cells:\n",
    "        try:\n",
    "            title = event_cell.select('h3 > strong')[0].text\n",
    "            rest = event_cell.find('time').text.split(',')\n",
    "            try:\n",
    "                place = rest[1]\n",
    "            except:\n",
    "                place = ''\n",
    "            try:\n",
    "                date = rest[0]\n",
    "            except:\n",
    "                date = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "        scraped_events_per_page.append((title, place, date))\n",
    "        \n",
    "    return scraped_events_per_page\n",
    "\n",
    "\n",
    "base_url = 'http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&ArrStartdato=20%2F7+2020&ArrSlutdato=3%2F8+2020&Genre=Musik'\n",
    "\n",
    "scraped_events = []\n",
    "indexes = list(range(1, int(no_of_events), 20))\n",
    "indexes[0] = 0\n",
    "\n",
    "for idx in tqdm(indexes):\n",
    "    scrape_url = base_url.format(idx)\n",
    "    #print(scrape_url)\n",
    "    scraped_events += scrape_events_per_page(scrape_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Københavns Kammerkor øver', ' Davidskirken', 'Mon 20 Jul 2020 6 pm'),\n",
       " ('Sommer i Haveselskabet: Signe Svendsen',\n",
       "  ' Mielcke & Hurtigkarl',\n",
       "  'Mon 20 Jul 2020 7 pm'),\n",
       " ('Copenhagen Gospel Days 2020', ' Templet', 'Tues 21 Jul 2020 2 pm'),\n",
       " ('Sommer I Haveselskabet: Mads Langer',\n",
       "  ' Det Kgl. Danske Haveselskabs Have',\n",
       "  'Tues 21 Jul 2020 7 pm'),\n",
       " ('Orgelkoncert med Jakob Lorentzen',\n",
       "  ' Holmens Kirke',\n",
       "  'Wed 22 Jul 2020 12 am'),\n",
       " ('Det Italienske Barokorgel',\n",
       "  ' Trinitatis Kirke',\n",
       "  'Wed 22 Jul 2020 2 pm  - 2.30 pm'),\n",
       " ('Alle de kvinder vi er', ' Sions Kirke', 'Wed 22 Jul 2020 5 pm'),\n",
       " ('Sommer I Haveselskabet: Steen Jørgensen',\n",
       "  ' Det Kgl. Danske Haveselskabs Have',\n",
       "  'Wed 22 Jul 2020 7 pm'),\n",
       " ('Copenhagen Gospel Days 2020', ' Templet', 'Thur 23 Jul 2020 2 pm'),\n",
       " ('Sommer i Haveselskabet: Barbara Moleko',\n",
       "  ' Mielcke & Hurtigkarl',\n",
       "  'Thur 23 Jul 2020 7 pm'),\n",
       " ('Sommer i Haveselskabet: Halberg & Friends',\n",
       "  ' Det Kgl. Danske Haveselskabs Have',\n",
       "  'Fri 24 Jul 2020 2 pm'),\n",
       " ('Nicklas Sørensen + Lund & Pultz Melbye // ALICE Summer Series',\n",
       "  ' Union',\n",
       "  'Fri 24 Jul 2020 5 pm'),\n",
       " ('Københavns Kammerkor øver', ' Davidskirken', 'Mon 20 Jul 2020 6 pm'),\n",
       " ('Sommer i Haveselskabet: Signe Svendsen',\n",
       "  ' Mielcke & Hurtigkarl',\n",
       "  'Mon 20 Jul 2020 7 pm'),\n",
       " ('Copenhagen Gospel Days 2020', ' Templet', 'Tues 21 Jul 2020 2 pm'),\n",
       " ('Sommer I Haveselskabet: Mads Langer',\n",
       "  ' Det Kgl. Danske Haveselskabs Have',\n",
       "  'Tues 21 Jul 2020 7 pm'),\n",
       " ('Orgelkoncert med Jakob Lorentzen',\n",
       "  ' Holmens Kirke',\n",
       "  'Wed 22 Jul 2020 12 am'),\n",
       " ('Det Italienske Barokorgel',\n",
       "  ' Trinitatis Kirke',\n",
       "  'Wed 22 Jul 2020 2 pm  - 2.30 pm'),\n",
       " ('Alle de kvinder vi er', ' Sions Kirke', 'Wed 22 Jul 2020 5 pm'),\n",
       " ('Sommer I Haveselskabet: Steen Jørgensen',\n",
       "  ' Det Kgl. Danske Haveselskabs Have',\n",
       "  'Wed 22 Jul 2020 7 pm')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_events[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise with beautifulSoup\n",
    "Use BeautifulSoup to extract all titles on all radio programs https://www.dr.dk/radio/programmer\n",
    "1. First find how many pages there are\n",
    "2. Then find all titles on https://www.dr.dk/radio/programmer?side=1\n",
    "3. Then find all titles on all pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to Extract Dates and Prices from Strings.\n",
    "\n",
    "Remember, the raw data, which we extracted from the web pages is all of type `str`. To do statistics about possible correlation of start times and entry fees, we need to convert the corresponding tuple fields into datetimes and integers respectively.\n",
    "\n",
    "\n",
    "Since dates given on the web do not necessarily conform to standardized time formats, we can apply the `dateparser` (https://pypi.python.org/pypi/dateparser) module, which tries to parse arbitrary strings into datetimes.\n",
    "\n",
    "You can install the module via:\n",
    "\n",
    "```bash\n",
    "pip install dateparser\n",
    "```\n",
    "\n",
    "You can read more about the module and its capabilities https://dateparser.readthedocs.io/en/latest/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dateparser\n",
      "  Downloading https://files.pythonhosted.org/packages/82/9d/51126ac615bbc4418478d725a5fa1a0f112059f6f111e4b48cfbe17ef9d0/dateparser-0.7.2-py2.py3-none-any.whl (352kB)\n",
      "Requirement already satisfied: pytz in /home/tha/.anaconda3/lib/python3.7/site-packages (from dateparser) (2019.3)\n",
      "Collecting tzlocal (from dateparser)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/99/53bd1ac9349262f59c1c421d8fcc2559ae8a5eeffed9202684756b648d33/tzlocal-2.0.0-py2.py3-none-any.whl\n",
      "Collecting regex (from dateparser)\n",
      "  Downloading https://files.pythonhosted.org/packages/97/6e/2f8d8e86621ab6eb84a2200c78b24343f74dd722f493a7fbbac37036dc13/regex-2020.1.8-cp37-cp37m-manylinux2010_x86_64.whl (690kB)\n",
      "Requirement already satisfied: python-dateutil in /home/tha/.anaconda3/lib/python3.7/site-packages (from dateparser) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/tha/.anaconda3/lib/python3.7/site-packages (from python-dateutil->dateparser) (1.12.0)\n",
      "Installing collected packages: tzlocal, regex, dateparser\n",
      "Successfully installed dateparser-0.7.2 regex-2020.1.8 tzlocal-2.0.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#pip install dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:02<00:00, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2020, 7, 20, 18, 0), datetime.datetime(2020, 7, 20, 19, 0), datetime.datetime(2020, 7, 21, 14, 0), datetime.datetime(2020, 7, 21, 19, 0), datetime.datetime(2020, 7, 22, 14, 0), datetime.datetime(2020, 7, 22, 17, 0), datetime.datetime(2020, 7, 22, 19, 0), datetime.datetime(2020, 7, 24, 14, 0), datetime.datetime(2020, 7, 24, 17, 0), datetime.datetime(2020, 7, 20, 18, 0), datetime.datetime(2020, 7, 20, 19, 0), datetime.datetime(2020, 7, 21, 14, 0), datetime.datetime(2020, 7, 21, 19, 0), datetime.datetime(2020, 7, 22, 14, 0), datetime.datetime(2020, 7, 22, 17, 0), datetime.datetime(2020, 7, 22, 19, 0), datetime.datetime(2020, 7, 24, 14, 0), datetime.datetime(2020, 7, 24, 17, 0), datetime.datetime(2020, 7, 20, 18, 0), datetime.datetime(2020, 7, 20, 19, 0), datetime.datetime(2020, 7, 21, 14, 0), datetime.datetime(2020, 7, 21, 19, 0), datetime.datetime(2020, 7, 22, 14, 0), datetime.datetime(2020, 7, 22, 17, 0), datetime.datetime(2020, 7, 22, 19, 0), datetime.datetime(2020, 7, 24, 14, 0), datetime.datetime(2020, 7, 24, 17, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "from dateparser import parse\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_dates_and_prices(scraped_events):\n",
    "    \"\"\"\n",
    "    NO LONGER WORKS WELL WITH KULTUNAUT website after they changes layout and hid the prices behind a js function.\n",
    "    Cleanup the data. Get price as integer and date as date.\n",
    "    \n",
    "    returns:\n",
    "        A two-element tuple with a datetime representing the start \n",
    "        time of an event and an integer representing the price in Dkk.\n",
    "    \"\"\"\n",
    "\n",
    "    price_regexp = r\"(?P<price>\\d+)\" #initial ? is a lookbehind. r() r is for raw text, P<some pattern> is to give a pattern name to refer to. \\d is numeric digit, + is for 1 or more.\n",
    "\n",
    "    data_points = []\n",
    "    three_at_night = datetime.now().replace(hour=3, minute=0, second=0, microsecond=0).time()\n",
    "    for event_data in tqdm(scraped_events):\n",
    "        title_str, place_str, date_str, price_str = event_data\n",
    "        \n",
    "        if 'Free admission' in price_str:\n",
    "            price = 0\n",
    "        else:\n",
    "            m = re.search(price_regexp, price_str) # m is the Match object returned from re.search (might be None)\n",
    "            try:\n",
    "                price = int(m.group('price')) # if price can be converted to int then we do it else return 0.\n",
    "            except:\n",
    "                price = 0\n",
    "\n",
    "        date_str = date_str.strip().strip('.')\n",
    "        if '&' in date_str:\n",
    "            date_str = date_str.split('&')[0]\n",
    "        if '-' in date_str:\n",
    "            date_str = date_str.split('-')[0]\n",
    "        if '.' in date_str:\n",
    "            date_str = date_str.replace('.', ':')\n",
    "        \n",
    "        date = parse(date_str)\n",
    "        if date and date.time() > three_at_night:\n",
    "            data_points.append((date, price))\n",
    "            \n",
    "    return data_points\n",
    "\n",
    "def get_dates(scraped_events):\n",
    "    \"\"\"\n",
    "    Cleanup the data. Get date as date.\n",
    "    \n",
    "    returns:\n",
    "        A datetime representing the start \n",
    "        time of an event.\n",
    "    \"\"\"\n",
    "    three_at_night = datetime.now().replace(hour=3, minute=0, second=0, microsecond=0).time()\n",
    "    dates = []\n",
    "    for event_data in tqdm(scraped_events):\n",
    "        title_str, place_str, date_str = event_data\n",
    "        \n",
    "        date_str = date_str.strip().strip('.')\n",
    "        if '&' in date_str:\n",
    "            date_str = date_str.split('&')[0]\n",
    "        if '-' in date_str:\n",
    "            date_str = date_str.split('-')[0]\n",
    "        if '.' in date_str:\n",
    "            date_str = date_str.replace('.', ':')\n",
    "        \n",
    "        date = parse(date_str)\n",
    "        if date and date.time() > three_at_night:\n",
    "            dates.append(date)\n",
    "    return dates\n",
    "\n",
    "dates = get_dates(scraped_events)\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2020, 7, 20, 19, 0),\n",
       " datetime.datetime(2020, 7, 21, 14, 0),\n",
       " datetime.datetime(2020, 7, 21, 19, 0),\n",
       " datetime.datetime(2020, 7, 22, 14, 0),\n",
       " datetime.datetime(2020, 7, 22, 17, 0),\n",
       " datetime.datetime(2020, 7, 22, 19, 0),\n",
       " datetime.datetime(2020, 7, 24, 14, 0),\n",
       " datetime.datetime(2020, 7, 24, 17, 0),\n",
       " datetime.datetime(2020, 7, 20, 18, 0),\n",
       " datetime.datetime(2020, 7, 20, 19, 0)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scraping Images from a Page\n",
    "\n",
    "In the following code you will use Beautiful Soup to extract all links to images, which are in `img` tags on a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQP9KlNgkVw1nYWndb2AB2CHr2_JUAA_4-G-cyKWrz2wPF285WCARQqsebZeg&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQeHlNBWWWjLJBkbmrV51AAVaAxSYMONttxZn6TtXI4DnRTgnKefTCxomPU5A&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSKl9k2Hu8uKp5FBuxBCztZrnd4kvA0-TIxMKlcZVopJ7HQPa8BMONioQf4bg&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTmOzg1QuD-2sMde9jVKs7iJUpgWHS50JBrrHbo7E57j4oW4Bw5M-4A6cf7zA&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSJtF88xAG2xpNJfyK8YJuYCO6Viz5tftw9zAuGd6USxcpuwfKf4bp-KsRiDzw&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQTrO7PBf2eVJp3pMg6oLsyuLoG-XEUaokc9obrLCX1V8EkUWapLeGU4eH2rP4&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR5H6UOXCW719eJwvLzNKuMivfjDxpB3DSzG0GpeubLo5J8JDHcOAfQXu1usA&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS8lkBOwaZ0cF2iEH_Favo8q6g7f--ojYSam2wxtUKutaGZLfZlaua_HFgcGD8&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQTeyE8-qPASkGPZVfif723riHCLa_eSbfBzio_w2mIBObckbfVUJ8D4R3UbxY&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQrZXaqKi4eMIBZOoDWv3_yXJBdRYAoxWoO7L_addJ42KU2o9wWyKyBj2XIag&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSaLMP3PTNNZrAVUT3nuhCDv2JqHO7Dqdo-vnTE_Ocuh4RcrJUpGX_VpS7VMeE&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT5JddRCXIzgmRc2TmyE06FMvu6ve4CwZnciJ8fvQ6evNPa6y4Xm3sMGpGwwQ&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSMmrB56vjxjHH0n3MTJpDjpdLJPc3zOauJuYmoTD8X9Qpu5229KKOu-YQdBg&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRv-HBCzdYDyC-xGMw9zxMheIJ6A2vRsU1FrJRyxH-4IxQ4wkgRejUZDclucQ&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQjhooi60VVP9_mWwXMFgnVy5Hq6PFadpKpQzIsFEPf3tqYcOKhSJ3MUioMO50&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSOBOBBwadtOJLBE8jw8CO82Yi8XpuHV3v2PEz_rCFTr7K-re6SVjNtfCs_uvY&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT2W7rmyGR1E1OyQ9Wdo3vQUdvF7Ut0JaluxLFb7lpUW0d358yXHlcVVti9nCQ&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSWoInAjdA_NJPHvLls9cBz5WdHpEqvtofl91KPJjhcUuJQJUjwtTUHpRXSyQ&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTPfJwhwJZHu_7VChodVCMH_LJAfrgoBz3rCPMSgG1DOeJVnxMWzmmZgJXQtBI&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS02yiZb-JkWuFpIuaJ6FUA9TH_U62C4kM5ypDRnEHM1K40Cw_JX378y9Jqni0&s']\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "\n",
    "def collect_img_links(url):\n",
    "    \"\"\"based on a url returns a list of image links contained in the requested page\"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #print(soup.select('img'))\n",
    "    return [img.get('src') for img in soup.select('img') \n",
    "            if img.get('src') and img.get('src').startswith('http')]\n",
    "\n",
    "\n",
    "def download_imgs(links, out_folder=\"./data/test/\"):\n",
    "    \"\"\"download all images from a list of image links. \n",
    "    Requires a folder named: test to be there\"\"\"\n",
    "    img_no = 0\n",
    "    for l in links:\n",
    "        img_no += 1\n",
    "        r = requests.get(l, stream=True)\n",
    "        with open(out_folder+'img'+str(img_no)+'.jpg', 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)     \n",
    "        \n",
    "links = collect_img_links('https://www.google.dk/search?site=&tbm=isch&source=hp&biw=1163&bih=812&q=minions&oq=minions')\n",
    "print(links)\n",
    "download_imgs(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 2: Writing a Simple Web Crawler\n",
    "\n",
    "Write a simple web crawler. More precisely, a program that extracts recursively all links from web pages. The result of running the web crawler is a dictionary, were the key-value pairs correspond to outgoiung links from a web page with the URL, which is stored in the key.\n",
    "\n",
    "\n",
    "In case a page returns a status code, which is not `200` we just disregard this page. See https://en.wikipedia.org/wiki/List_of_HTTP_status_codes for more detailes on the various HTTP status codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_links(from_url, for_depth, all_links={}):\n",
    "    # This is what the exercise below asks you to implement!\n",
    "    pass\n",
    "\n",
    "\n",
    "start_url = 'https://www.version2.dk/artikel/google-deepmind-vi-oeger-sikkerheden-mod-misbrug-sundhedsdata-1074452'\n",
    "\n",
    "link_dict = scrape_links(from_url=start_url, for_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The web crawler that you wrote above is perhaps not the most performant. If you are interested in more web scraping and application of crawlers have a look at the `scrapy` module (https://scrapy.org)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
